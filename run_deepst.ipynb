{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c9bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import deepstkit as dt\n",
    "import torch\n",
    "\n",
    "SEED = 0                     # Random seed for reproducibility\n",
    "DATA_DIR = \"/scratch/harsha.vasamsetti/DeepST/data/DLPFC\"   # Directory containing spatial data\n",
    "SAMPLE_ID = \"151673\"         # Sample identifier to analyze\n",
    "RESULTS_DIR = \"./Results\"   # Directory to save outputs\n",
    "N_DOMAINS = 7                # Expected number of spatial domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fcd916-cfea-4b26-a442-0ae6b9557381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/harsha.vasamsetti/DeepST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial weights calculated. Average neighbors: 30.0\n",
      "Gene expression weights calculated.\n",
      "Final weight matrix calculated and stored in adata.obsm['weights_matrix_all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding adjacent spots:  26%|██▌        [ time left: 00:01 ]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# ========== Initialize Analysis ==========\n",
    "# Set random seed and initialize DeepST\n",
    "# dt.utils_func.seed_torch(seed=SEED)\n",
    "\n",
    "# Create DeepST instance with analysis parameters\n",
    "deepst = dt.main.run(\n",
    "    save_path=RESULTS_DIR,\n",
    "    task=\"Identify_Domain\",  # Spatial domain identification\n",
    "    pre_epochs=500,          # Pretraining iterations\n",
    "    epochs=500,              # Main training iterations\n",
    "    use_gpu=True             # Accelerate with GPU if available\n",
    ")\n",
    "\n",
    "# ========== Data Loading & Preprocessing ==========\n",
    "# (Optional) Load spatial transcriptomics data (Visium platform)\n",
    "# e.g. adata = anndata.read_h5ad(\"*.h5ad\"), this data including .obsm['spatial']\n",
    "adata = deepst._get_adata(\n",
    "    platform=\"Visium\",\n",
    "    data_path=DATA_DIR,\n",
    "    data_name=SAMPLE_ID\n",
    ")\n",
    "\n",
    "# Optional: Incorporate H&E image features (skip if not available)\n",
    "# adata = deepst._get_image_crop(adata, data_name=SAMPLE_ID)\n",
    "\n",
    "# ========== Feature Engineering ==========\n",
    "# Data augmentation (skip morphological if no H&E)\n",
    "adata = deepst._get_augment(\n",
    "    adata,\n",
    "    spatial_type=\"BallTree\",\n",
    "    use_morphological = False  # Set True if using H&E features\n",
    ")\n",
    "\n",
    "# Construct spatial neighborhood graph\n",
    "graph_dict = deepst._get_graph(\n",
    "    adata.obsm[\"spatial\"],\n",
    "    distType=\"KDTree\"        # Spatial relationship modeling\n",
    ")\n",
    "\n",
    "# Dimensionality reduction\n",
    "data = deepst._data_process(\n",
    "    adata,\n",
    "    pca_n_comps=200          # Reduce to 200 principal components\n",
    ")\n",
    "\n",
    "# ========== Model Training ==========\n",
    "# Train DeepST model and obtain embeddings\n",
    "deepst_embed, attention_data = deepst._fit(\n",
    "    data=data,\n",
    "    graph_dict=graph_dict,\n",
    "    conv_type=\"GATConv\"\n",
    ")\n",
    "adata.obsm[\"DeepST_embed\"] = deepst_embed\n",
    "adata.obsm[\"gat_attention\"] = {'edge_index': attention_data[0], 'attention_scores': attention_data[1]}\n",
    "\n",
    "# ========== Spatial Domain Detection ==========\n",
    "# Cluster spots into spatial domains\n",
    "adata = deepst._get_cluster_data(\n",
    "    adata,\n",
    "    n_domains=N_DOMAINS,     # Expected number of domains\n",
    "    priori=True              # Use prior knowledge if available\n",
    ")\n",
    "\n",
    "# ========== Visualization & Output ==========\n",
    "# Plot spatial domains\n",
    "sc.pl.spatial(\n",
    "    adata,\n",
    "    color=[\"DeepST_refine_domain\"],  # Color by domain\n",
    "    frameon=False,\n",
    "    spot_size=150,\n",
    "    title=f\"Spatial Domains - {SAMPLE_ID}\"\n",
    ")\n",
    "\n",
    "# Save results\n",
    "output_file = os.path.join(RESULTS_DIR, f\"{SAMPLE_ID}_domains.pdf\")\n",
    "plt.savefig(output_file, bbox_inches=\"tight\", dpi=300)\n",
    "print(f\"Analysis complete! Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7j9nisl0mjk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import torch\n",
    "\n",
    "def visualize_gat_attention(\n",
    "    adata,\n",
    "    top_k=100,\n",
    "    min_attention_threshold=0.1,\n",
    "    figsize=(12, 10),\n",
    "    spot_size=30,\n",
    "    edge_alpha_scale=3.0,\n",
    "    edge_width_scale=2.0,\n",
    "    colormap='viridis',\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize GAT attention weights on spatial transcriptomics data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        Annotated data with spatial coordinates and attention weights\n",
    "    top_k : int\n",
    "        Number of top attention edges to visualize (default: 100)\n",
    "    min_attention_threshold : float\n",
    "        Minimum attention score to display (default: 0.1)\n",
    "    figsize : tuple\n",
    "        Figure size (default: (12, 10))\n",
    "    spot_size : int\n",
    "        Size of spatial spots (default: 30)\n",
    "    edge_alpha_scale : float\n",
    "        Scale factor for edge transparency (default: 3.0)\n",
    "    edge_width_scale : float\n",
    "        Scale factor for edge width (default: 2.0)\n",
    "    colormap : str\n",
    "        Colormap for attention weights (default: 'viridis')\n",
    "    save_path : str, optional\n",
    "        Path to save the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if attention data exists\n",
    "    if 'gat_attention' not in adata.obsm:\n",
    "        print(\"No GAT attention weights found. Make sure Conv_type='GATConv' in model.\")\n",
    "        return\n",
    "    \n",
    "    attention_data = adata.obsm['gat_attention']\n",
    "    edge_index = attention_data['edge_index']\n",
    "    attention_scores = attention_data['attention_scores']\n",
    "    \n",
    "    # Check if we have valid attention data\n",
    "    if edge_index is None or attention_scores is None:\n",
    "        print(\"Attention weights are None. This may happen with non-GAT convolutions.\")\n",
    "        return\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    spatial_coords = adata.obsm['spatial']\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(edge_index, torch.Tensor):\n",
    "        edge_index = edge_index.numpy()\n",
    "    if isinstance(attention_scores, torch.Tensor):\n",
    "        attention_scores = attention_scores.numpy()\n",
    "    \n",
    "    # Get the number of attention heads (if multi-head)\n",
    "    if len(attention_scores.shape) > 1:\n",
    "        # Average across attention heads\n",
    "        attention_scores = attention_scores.mean(axis=1)\n",
    "    \n",
    "    # Filter edges by top-k and threshold\n",
    "    attention_mask = attention_scores > min_attention_threshold\n",
    "    if top_k is not None and top_k < len(attention_scores):\n",
    "        # Get indices of top-k attention scores\n",
    "        top_k_indices = np.argpartition(attention_scores, -top_k)[-top_k:]\n",
    "        mask = np.zeros_like(attention_scores, dtype=bool)\n",
    "        mask[top_k_indices] = True\n",
    "        attention_mask = attention_mask & mask\n",
    "    \n",
    "    # Get filtered edges\n",
    "    filtered_edges = edge_index[:, attention_mask]\n",
    "    filtered_scores = attention_scores[attention_mask]\n",
    "    \n",
    "    if len(filtered_scores) == 0:\n",
    "        print(f\"No edges above threshold {min_attention_threshold}. Try lowering the threshold.\")\n",
    "        return\n",
    "    \n",
    "    # Normalize attention scores for visualization\n",
    "    norm_scores = (filtered_scores - filtered_scores.min()) / (filtered_scores.max() - filtered_scores.min() + 1e-8)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: Attention network on spatial coordinates\n",
    "    ax1.set_title(f'GAT Attention Weights (Top {len(filtered_scores)} edges)', fontsize=14)\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    # Draw edges with attention weights\n",
    "    segments = []\n",
    "    for i in range(filtered_edges.shape[1]):\n",
    "        source = filtered_edges[0, i]\n",
    "        target = filtered_edges[1, i]\n",
    "        segments.append([spatial_coords[source], spatial_coords[target]])\n",
    "    \n",
    "    # Create line collection with attention-based properties\n",
    "    lc = LineCollection(\n",
    "        segments,\n",
    "        linewidths=norm_scores * edge_width_scale,\n",
    "        alpha=norm_scores * edge_alpha_scale,\n",
    "        cmap=colormap\n",
    "    )\n",
    "    lc.set_array(filtered_scores)\n",
    "    ax1.add_collection(lc)\n",
    "    \n",
    "    # Plot spots\n",
    "    ax1.scatter(\n",
    "        spatial_coords[:, 0],\n",
    "        spatial_coords[:, 1],\n",
    "        c='lightgray',\n",
    "        s=spot_size,\n",
    "        edgecolors='black',\n",
    "        linewidths=0.5,\n",
    "        zorder=2\n",
    "    )\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(lc, ax=ax1, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Attention Score', rotation=270, labelpad=15)\n",
    "    \n",
    "    ax1.set_xlabel('Spatial X')\n",
    "    ax1.set_ylabel('Spatial Y')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Attention score distribution\n",
    "    ax2.set_title('Attention Score Distribution', fontsize=14)\n",
    "    ax2.hist(attention_scores, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(min_attention_threshold, color='red', linestyle='--', label=f'Threshold: {min_attention_threshold}')\n",
    "    ax2.set_xlabel('Attention Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    textstr = f'Mean: {attention_scores.mean():.3f}\\n'\n",
    "    textstr += f'Std: {attention_scores.std():.3f}\\n'\n",
    "    textstr += f'Max: {attention_scores.max():.3f}\\n'\n",
    "    textstr += f'Min: {attention_scores.min():.3f}'\n",
    "    ax2.text(0.65, 0.95, textstr, transform=ax2.transAxes, fontsize=10,\n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Attention visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== Attention Weight Summary ===\")\n",
    "    print(f\"Total edges: {len(attention_scores)}\")\n",
    "    print(f\"Displayed edges: {len(filtered_scores)}\")\n",
    "    print(f\"Attention range: [{attention_scores.min():.4f}, {attention_scores.max():.4f}]\")\n",
    "    print(f\"Mean attention: {attention_scores.mean():.4f} ± {attention_scores.std():.4f}\")\n",
    "\n",
    "# Visualize the attention weights\n",
    "if 'gat_attention' in adata.obsm:\n",
    "    visualize_gat_attention(\n",
    "        adata,\n",
    "        top_k=150,  # Show top 150 attention connections\n",
    "        min_attention_threshold=0.05,  # Minimum attention score to display\n",
    "        figsize=(16, 8),\n",
    "        save_path=os.path.join(RESULTS_DIR, f\"{SAMPLE_ID}_attention_weights.pdf\")\n",
    "    )\n",
    "else:\n",
    "    print(\"No attention weights found. Make sure to use Conv_type='GATConv' when training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c4ae1-f255-4559-a03c-c8d4c394fc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_csv from `anndata` is deprecated. Import anndata.io.read_csv instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_excel from `anndata` is deprecated. Import anndata.io.read_excel instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_hdf from `anndata` is deprecated. Import anndata.io.read_hdf instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_loom from `anndata` is deprecated. Import anndata.io.read_loom instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_mtx from `anndata` is deprecated. Import anndata.io.read_mtx instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_umi_tools from `anndata` is deprecated. Import anndata.io.read_umi_tools instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/louvain/__init__.py:54: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial weights calculated. Average neighbors: 30.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import deepstkit as dt\n",
    "\n",
    "SEED = 0  \n",
    "DATA_DIR = \"./data/DLPFC\"        \n",
    "SAMPLE_IDS = ['151673', '151674','151675', '151676']\n",
    "RESULTS_DIR = \"./Results\"        \n",
    "N_DOMAINS = 7                             \n",
    "INTEGRATION_NAME = \"_\".join(SAMPLE_IDS)\n",
    "\n",
    "# Set random seed and initialize DeepST\n",
    "# dt.utils_func.seed_torch(seed=SEED)\n",
    "\n",
    "integration_model = dt.main.run(\n",
    "    save_path=RESULTS_DIR,\n",
    "    task=\"Integration\",       # Multi-sample integration task\n",
    "    pre_epochs=500,           \n",
    "    epochs=500,              \n",
    "    use_gpu=True              \n",
    ")\n",
    "\n",
    "processed_data = []\n",
    "spatial_graphs = []\n",
    "\n",
    "for sample_id in SAMPLE_IDS:\n",
    "    # Load and preprocess each sample\n",
    "    adata = integration_model._get_adata(\n",
    "        platform=\"Visium\",\n",
    "        data_path=DATA_DIR,\n",
    "        data_name=sample_id\n",
    "    )\n",
    "    \n",
    "    # Incorporate H&E image features (Optional)\n",
    "    # adata = integration_model._get_image_crop(adata, data_name=sample_id)\n",
    "    \n",
    "    # Feature augmentation\n",
    "    adata = integration_model._get_augment(\n",
    "        adata,\n",
    "        spatial_type=\"BallTree\",\n",
    "        use_morphological=False, # Use prior knowledge if available\n",
    "    )\n",
    "    \n",
    "    # Construct spatial neighborhood graph\n",
    "    graph = integration_model._get_graph(\n",
    "        adata.obsm[\"spatial\"],\n",
    "        distType=\"KDTree\"\n",
    "    )\n",
    "    \n",
    "    processed_data.append(adata)\n",
    "    spatial_graphs.append(graph)\n",
    "\n",
    "# Combine multiple samples into integrated dataset\n",
    "combined_adata, combined_graph = integration_model._get_multiple_adata(\n",
    "    adata_list=processed_data,\n",
    "    data_name_list=SAMPLE_IDS,\n",
    "    graph_list=spatial_graphs\n",
    ")\n",
    "\n",
    "# Dimensionality reduction\n",
    "integrated_data = integration_model._data_process(\n",
    "    combined_adata,\n",
    "    pca_n_comps=200\n",
    ")\n",
    "\n",
    "# Train with domain adversarial learning\n",
    "embeddings = integration_model._fit(\n",
    "    data=integrated_data,\n",
    "    graph_dict=combined_graph,\n",
    "    domains=combined_adata.obs[\"batch\"].values,  # For batch correction\n",
    "    n_domains=len(SAMPLE_IDS) )                 # Number of batches\n",
    "\n",
    "combined_adata.obsm[\"DeepST_embed\"] = embeddings\n",
    "\n",
    "combined_adata = integration_model._get_cluster_data(\n",
    "    combined_adata,\n",
    "    n_domains=N_DOMAINS,\n",
    "    priori=True,             # Use biological priors if available\n",
    "    batch_key=\"batch_name\",\n",
    ")\n",
    "\n",
    "# UMAP of integrated data\n",
    "sc.pp.neighbors(combined_adata, use_rep='DeepST_embed')\n",
    "sc.tl.umap(combined_adata)\n",
    "\n",
    "# Save combined UMAP plot\n",
    "umap_plot = sc.pl.umap(\n",
    "    combined_adata,\n",
    "    color=[\"DeepST_refine_domain\", \"batch_name\"],\n",
    "    title=f\"Integrated UMAP - Samples {INTEGRATION_NAME}\",\n",
    "    return_fig=True\n",
    ")\n",
    "umap_plot.savefig(\n",
    "    os.path.join(RESULTS_DIR, f\"{INTEGRATION_NAME}_integrated_umap.pdf\"),\n",
    "    bbox_inches='tight',\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Save individual spatial domain plots\n",
    "for sample_id in SAMPLE_IDS:\n",
    "    sample_data = combined_adata[combined_adata.obs[\"batch_name\"]==sample_id]\n",
    "    \n",
    "    spatial_plot = sc.pl.spatial(\n",
    "        sample_data,\n",
    "        color='DeepST_refine_domain',\n",
    "        title=f\"Spatial Domains - {sample_id}\",\n",
    "        frameon=False,\n",
    "        spot_size=150,\n",
    "        return_fig=True\n",
    "    )\n",
    "    spatial_plot.savefig(\n",
    "        os.path.join(RESULTS_DIR, f\"{sample_id}_domains.pdf\"),\n",
    "        bbox_inches='tight',\n",
    "        dpi=300\n",
    "    )\n",
    "\n",
    "print(f\"Integration complete! Results saved to {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
