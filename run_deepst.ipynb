{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c9bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import deepstkit as dt\n",
    "import torch\n",
    "\n",
    "SEED = 0                     # Random seed for reproducibility\n",
    "DATA_DIR = \"/scratch/harsha.vasamsetti/DeepST/data/DLPFC\"   # Directory containing spatial data\n",
    "SAMPLE_ID = \"151673\"         # Sample identifier to analyze\n",
    "RESULTS_DIR = \"./Results\"   # Directory to save outputs\n",
    "N_DOMAINS = 7                # Expected number of spatial domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fcd916-cfea-4b26-a442-0ae6b9557381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/harsha.vasamsetti/DeepST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial weights calculated. Average neighbors: 30.0\n",
      "Gene expression weights calculated.\n",
      "Final weight matrix calculated and stored in adata.obsm['weights_matrix_all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding adjacent spots: 100%|██████████ [ time left: 00:00 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data augmentation completed\n",
      "12.0000 neighbors per cell on average.\n",
      "Step 2: Spatial graph computation completed\n",
      "Running DeepST analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretraining initial model:   0%|           [ time left: ? ]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparseTensor' object has no attribute 'to_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m\n\u001b[1;32m     44\u001b[0m data \u001b[38;5;241m=\u001b[39m deepst\u001b[38;5;241m.\u001b[39m_data_process(\n\u001b[1;32m     45\u001b[0m     adata,\n\u001b[1;32m     46\u001b[0m     pca_n_comps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m          \u001b[38;5;66;03m# Reduce to 200 principal components\u001b[39;00m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# ========== Model Training ==========\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Train DeepST model and obtain embeddings\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m deepst_embed, attention_data \u001b[38;5;241m=\u001b[39m \u001b[43mdeepst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGATConv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     55\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m adata\u001b[38;5;241m.\u001b[39mobsm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepST_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m deepst_embed\n\u001b[1;32m     57\u001b[0m adata\u001b[38;5;241m.\u001b[39mobsm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgat_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_data[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_scores\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_data[\u001b[38;5;241m1\u001b[39m]}\n",
      "File \u001b[0;32m/scratch/harsha.vasamsetti/DeepST/deepstkit/main.py:485\u001b[0m, in \u001b[0;36mrun._fit\u001b[0;34m(self, data, graph_dict, domains, n_domains, conv_type, linear_encoder_hidden, linear_decoder_hidden, conv_hidden, p_drop, dec_cluster_n, kl_weight, mse_weight, bce_kld_weight, domain_weight)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m embeddings, _, attention_data \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mprocess()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Print stats\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/harsha.vasamsetti/DeepST/deepstkit/trainer.py:254\u001b[0m, in \u001b[0;36mtrain.fit\u001b[0;34m(self, cluster_n, cluster_type, resolution, pretrain)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Pretrain if specified\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrain:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     pre_z, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess()\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Initialize cluster centers\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/harsha.vasamsetti/DeepST/deepstkit/trainer.py:134\u001b[0m, in \u001b[0;36mtrain.pretrain\u001b[0;34m(self, grad_clip)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomains \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     z, mu, logvar, de_feat, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdc(z)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/harsha.vasamsetti/DeepST/deepstkit/model.py:343\u001b[0m, in \u001b[0;36mDeepST_model.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03mForward pass of DeepST model\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m        attention weights from the GAT layer\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# mu, logvar, feat_x = self.encode(x, adj)\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m mu, logvar, feat_x, attention_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m gnn_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[1;32m    345\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((feat_x, gnn_z), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/harsha.vasamsetti/DeepST/deepstkit/model.py:198\u001b[0m, in \u001b[0;36mDeepST_model.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Handle attention weights only for GATConv\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGATConv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# Convert sparsetensor to edge index for GATconv\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     edge_index, _ \u001b[38;5;241m=\u001b[39m \u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_edge_index\u001b[49m()\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Manually call layers to capture attention with return_attention_weights parameter\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     conv_x, attention_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(feat_x, edge_index, return_attention_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparseTensor' object has no attribute 'to_edge_index'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# ========== Initialize Analysis ==========\n",
    "# Set random seed and initialize DeepST\n",
    "# dt.utils_func.seed_torch(seed=SEED)\n",
    "\n",
    "# Create DeepST instance with analysis parameters\n",
    "deepst = dt.main.run(\n",
    "    save_path=RESULTS_DIR,\n",
    "    task=\"Identify_Domain\",  # Spatial domain identification\n",
    "    pre_epochs=500,          # Pretraining iterations\n",
    "    epochs=500,              # Main training iterations\n",
    "    use_gpu=True             # Accelerate with GPU if available\n",
    ")\n",
    "\n",
    "# ========== Data Loading & Preprocessing ==========\n",
    "# (Optional) Load spatial transcriptomics data (Visium platform)\n",
    "# e.g. adata = anndata.read_h5ad(\"*.h5ad\"), this data including .obsm['spatial']\n",
    "adata = deepst._get_adata(\n",
    "    platform=\"Visium\",\n",
    "    data_path=DATA_DIR,\n",
    "    data_name=SAMPLE_ID\n",
    ")\n",
    "\n",
    "# Optional: Incorporate H&E image features (skip if not available)\n",
    "# adata = deepst._get_image_crop(adata, data_name=SAMPLE_ID)\n",
    "\n",
    "# ========== Feature Engineering ==========\n",
    "# Data augmentation (skip morphological if no H&E)\n",
    "adata = deepst._get_augment(\n",
    "    adata,\n",
    "    spatial_type=\"BallTree\",\n",
    "    use_morphological = False  # Set True if using H&E features\n",
    ")\n",
    "\n",
    "# Construct spatial neighborhood graph\n",
    "graph_dict = deepst._get_graph(\n",
    "    adata.obsm[\"spatial\"],\n",
    "    distType=\"KDTree\"        # Spatial relationship modeling\n",
    ")\n",
    "\n",
    "# Dimensionality reduction\n",
    "data = deepst._data_process(\n",
    "    adata,\n",
    "    pca_n_comps=200          # Reduce to 200 principal components\n",
    ")\n",
    "\n",
    "# ========== Model Training ==========\n",
    "# Train DeepST model and obtain embeddings\n",
    "deepst_embed, attention_data = deepst._fit(\n",
    "    data=data,\n",
    "    graph_dict=graph_dict,\n",
    "    conv_type=\"GATConv\"\n",
    ")\n",
    "adata.obsm[\"DeepST_embed\"] = deepst_embed\n",
    "adata.obsm[\"gat_attention\"] = {'edge_index': attention_data[0], 'attention_scores': attention_data[1]}\n",
    "\n",
    "# ========== Spatial Domain Detection ==========\n",
    "# Cluster spots into spatial domains\n",
    "adata = deepst._get_cluster_data(\n",
    "    adata,\n",
    "    n_domains=N_DOMAINS,     # Expected number of domains\n",
    "    priori=True              # Use prior knowledge if available\n",
    ")\n",
    "\n",
    "# ========== Visualization & Output ==========\n",
    "# Plot spatial domains\n",
    "sc.pl.spatial(\n",
    "    adata,\n",
    "    color=[\"DeepST_refine_domain\"],  # Color by domain\n",
    "    frameon=False,\n",
    "    spot_size=150,\n",
    "    title=f\"Spatial Domains - {SAMPLE_ID}\"\n",
    ")\n",
    "\n",
    "# Save results\n",
    "output_file = os.path.join(RESULTS_DIR, f\"{SAMPLE_ID}_domains.pdf\")\n",
    "plt.savefig(output_file, bbox_inches=\"tight\", dpi=300)\n",
    "print(f\"Analysis complete! Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7j9nisl0mjk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import torch\n",
    "\n",
    "def visualize_gat_attention(\n",
    "    adata,\n",
    "    top_k=100,\n",
    "    min_attention_threshold=0.1,\n",
    "    figsize=(12, 10),\n",
    "    spot_size=30,\n",
    "    edge_alpha_scale=3.0,\n",
    "    edge_width_scale=2.0,\n",
    "    colormap='viridis',\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize GAT attention weights on spatial transcriptomics data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        Annotated data with spatial coordinates and attention weights\n",
    "    top_k : int\n",
    "        Number of top attention edges to visualize (default: 100)\n",
    "    min_attention_threshold : float\n",
    "        Minimum attention score to display (default: 0.1)\n",
    "    figsize : tuple\n",
    "        Figure size (default: (12, 10))\n",
    "    spot_size : int\n",
    "        Size of spatial spots (default: 30)\n",
    "    edge_alpha_scale : float\n",
    "        Scale factor for edge transparency (default: 3.0)\n",
    "    edge_width_scale : float\n",
    "        Scale factor for edge width (default: 2.0)\n",
    "    colormap : str\n",
    "        Colormap for attention weights (default: 'viridis')\n",
    "    save_path : str, optional\n",
    "        Path to save the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if attention data exists\n",
    "    if 'gat_attention' not in adata.obsm:\n",
    "        print(\"No GAT attention weights found. Make sure Conv_type='GATConv' in model.\")\n",
    "        return\n",
    "    \n",
    "    attention_data = adata.obsm['gat_attention']\n",
    "    edge_index = attention_data['edge_index']\n",
    "    attention_scores = attention_data['attention_scores']\n",
    "    \n",
    "    # Check if we have valid attention data\n",
    "    if edge_index is None or attention_scores is None:\n",
    "        print(\"Attention weights are None. This may happen with non-GAT convolutions.\")\n",
    "        return\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    spatial_coords = adata.obsm['spatial']\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(edge_index, torch.Tensor):\n",
    "        edge_index = edge_index.numpy()\n",
    "    if isinstance(attention_scores, torch.Tensor):\n",
    "        attention_scores = attention_scores.numpy()\n",
    "    \n",
    "    # Get the number of attention heads (if multi-head)\n",
    "    if len(attention_scores.shape) > 1:\n",
    "        # Average across attention heads\n",
    "        attention_scores = attention_scores.mean(axis=1)\n",
    "    \n",
    "    # Filter edges by top-k and threshold\n",
    "    attention_mask = attention_scores > min_attention_threshold\n",
    "    if top_k is not None and top_k < len(attention_scores):\n",
    "        # Get indices of top-k attention scores\n",
    "        top_k_indices = np.argpartition(attention_scores, -top_k)[-top_k:]\n",
    "        mask = np.zeros_like(attention_scores, dtype=bool)\n",
    "        mask[top_k_indices] = True\n",
    "        attention_mask = attention_mask & mask\n",
    "    \n",
    "    # Get filtered edges\n",
    "    filtered_edges = edge_index[:, attention_mask]\n",
    "    filtered_scores = attention_scores[attention_mask]\n",
    "    \n",
    "    if len(filtered_scores) == 0:\n",
    "        print(f\"No edges above threshold {min_attention_threshold}. Try lowering the threshold.\")\n",
    "        return\n",
    "    \n",
    "    # Normalize attention scores for visualization\n",
    "    norm_scores = (filtered_scores - filtered_scores.min()) / (filtered_scores.max() - filtered_scores.min() + 1e-8)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: Attention network on spatial coordinates\n",
    "    ax1.set_title(f'GAT Attention Weights (Top {len(filtered_scores)} edges)', fontsize=14)\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    # Draw edges with attention weights\n",
    "    segments = []\n",
    "    for i in range(filtered_edges.shape[1]):\n",
    "        source = filtered_edges[0, i]\n",
    "        target = filtered_edges[1, i]\n",
    "        segments.append([spatial_coords[source], spatial_coords[target]])\n",
    "    \n",
    "    # Create line collection with attention-based properties\n",
    "    lc = LineCollection(\n",
    "        segments,\n",
    "        linewidths=norm_scores * edge_width_scale,\n",
    "        alpha=norm_scores * edge_alpha_scale,\n",
    "        cmap=colormap\n",
    "    )\n",
    "    lc.set_array(filtered_scores)\n",
    "    ax1.add_collection(lc)\n",
    "    \n",
    "    # Plot spots\n",
    "    ax1.scatter(\n",
    "        spatial_coords[:, 0],\n",
    "        spatial_coords[:, 1],\n",
    "        c='lightgray',\n",
    "        s=spot_size,\n",
    "        edgecolors='black',\n",
    "        linewidths=0.5,\n",
    "        zorder=2\n",
    "    )\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(lc, ax=ax1, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Attention Score', rotation=270, labelpad=15)\n",
    "    \n",
    "    ax1.set_xlabel('Spatial X')\n",
    "    ax1.set_ylabel('Spatial Y')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Attention score distribution\n",
    "    ax2.set_title('Attention Score Distribution', fontsize=14)\n",
    "    ax2.hist(attention_scores, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(min_attention_threshold, color='red', linestyle='--', label=f'Threshold: {min_attention_threshold}')\n",
    "    ax2.set_xlabel('Attention Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    textstr = f'Mean: {attention_scores.mean():.3f}\\n'\n",
    "    textstr += f'Std: {attention_scores.std():.3f}\\n'\n",
    "    textstr += f'Max: {attention_scores.max():.3f}\\n'\n",
    "    textstr += f'Min: {attention_scores.min():.3f}'\n",
    "    ax2.text(0.65, 0.95, textstr, transform=ax2.transAxes, fontsize=10,\n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Attention visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== Attention Weight Summary ===\")\n",
    "    print(f\"Total edges: {len(attention_scores)}\")\n",
    "    print(f\"Displayed edges: {len(filtered_scores)}\")\n",
    "    print(f\"Attention range: [{attention_scores.min():.4f}, {attention_scores.max():.4f}]\")\n",
    "    print(f\"Mean attention: {attention_scores.mean():.4f} ± {attention_scores.std():.4f}\")\n",
    "\n",
    "# Visualize the attention weights\n",
    "if 'gat_attention' in adata.obsm:\n",
    "    visualize_gat_attention(\n",
    "        adata,\n",
    "        top_k=150,  # Show top 150 attention connections\n",
    "        min_attention_threshold=0.05,  # Minimum attention score to display\n",
    "        figsize=(16, 8),\n",
    "        save_path=os.path.join(RESULTS_DIR, f\"{SAMPLE_ID}_attention_weights.pdf\")\n",
    "    )\n",
    "else:\n",
    "    print(\"No attention weights found. Make sure to use Conv_type='GATConv' when training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c4ae1-f255-4559-a03c-c8d4c394fc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_csv from `anndata` is deprecated. Import anndata.io.read_csv instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_excel from `anndata` is deprecated. Import anndata.io.read_excel instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_hdf from `anndata` is deprecated. Import anndata.io.read_hdf instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_loom from `anndata` is deprecated. Import anndata.io.read_loom instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_mtx from `anndata` is deprecated. Import anndata.io.read_mtx instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_umi_tools from `anndata` is deprecated. Import anndata.io.read_umi_tools instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/louvain/__init__.py:54: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/scratch/harsha.vasamsetti/sample_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial weights calculated. Average neighbors: 30.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import deepstkit as dt\n",
    "\n",
    "SEED = 0  \n",
    "DATA_DIR = \"./data/DLPFC\"        \n",
    "SAMPLE_IDS = ['151673', '151674','151675', '151676']\n",
    "RESULTS_DIR = \"./Results\"        \n",
    "N_DOMAINS = 7                             \n",
    "INTEGRATION_NAME = \"_\".join(SAMPLE_IDS)\n",
    "\n",
    "# Set random seed and initialize DeepST\n",
    "# dt.utils_func.seed_torch(seed=SEED)\n",
    "\n",
    "integration_model = dt.main.run(\n",
    "    save_path=RESULTS_DIR,\n",
    "    task=\"Integration\",       # Multi-sample integration task\n",
    "    pre_epochs=500,           \n",
    "    epochs=500,              \n",
    "    use_gpu=True              \n",
    ")\n",
    "\n",
    "processed_data = []\n",
    "spatial_graphs = []\n",
    "\n",
    "for sample_id in SAMPLE_IDS:\n",
    "    # Load and preprocess each sample\n",
    "    adata = integration_model._get_adata(\n",
    "        platform=\"Visium\",\n",
    "        data_path=DATA_DIR,\n",
    "        data_name=sample_id\n",
    "    )\n",
    "    \n",
    "    # Incorporate H&E image features (Optional)\n",
    "    # adata = integration_model._get_image_crop(adata, data_name=sample_id)\n",
    "    \n",
    "    # Feature augmentation\n",
    "    adata = integration_model._get_augment(\n",
    "        adata,\n",
    "        spatial_type=\"BallTree\",\n",
    "        use_morphological=False, # Use prior knowledge if available\n",
    "    )\n",
    "    \n",
    "    # Construct spatial neighborhood graph\n",
    "    graph = integration_model._get_graph(\n",
    "        adata.obsm[\"spatial\"],\n",
    "        distType=\"KDTree\"\n",
    "    )\n",
    "    \n",
    "    processed_data.append(adata)\n",
    "    spatial_graphs.append(graph)\n",
    "\n",
    "# Combine multiple samples into integrated dataset\n",
    "combined_adata, combined_graph = integration_model._get_multiple_adata(\n",
    "    adata_list=processed_data,\n",
    "    data_name_list=SAMPLE_IDS,\n",
    "    graph_list=spatial_graphs\n",
    ")\n",
    "\n",
    "# Dimensionality reduction\n",
    "integrated_data = integration_model._data_process(\n",
    "    combined_adata,\n",
    "    pca_n_comps=200\n",
    ")\n",
    "\n",
    "# Train with domain adversarial learning\n",
    "embeddings = integration_model._fit(\n",
    "    data=integrated_data,\n",
    "    graph_dict=combined_graph,\n",
    "    domains=combined_adata.obs[\"batch\"].values,  # For batch correction\n",
    "    n_domains=len(SAMPLE_IDS) )                 # Number of batches\n",
    "\n",
    "combined_adata.obsm[\"DeepST_embed\"] = embeddings\n",
    "\n",
    "combined_adata = integration_model._get_cluster_data(\n",
    "    combined_adata,\n",
    "    n_domains=N_DOMAINS,\n",
    "    priori=True,             # Use biological priors if available\n",
    "    batch_key=\"batch_name\",\n",
    ")\n",
    "\n",
    "# UMAP of integrated data\n",
    "sc.pp.neighbors(combined_adata, use_rep='DeepST_embed')\n",
    "sc.tl.umap(combined_adata)\n",
    "\n",
    "# Save combined UMAP plot\n",
    "umap_plot = sc.pl.umap(\n",
    "    combined_adata,\n",
    "    color=[\"DeepST_refine_domain\", \"batch_name\"],\n",
    "    title=f\"Integrated UMAP - Samples {INTEGRATION_NAME}\",\n",
    "    return_fig=True\n",
    ")\n",
    "umap_plot.savefig(\n",
    "    os.path.join(RESULTS_DIR, f\"{INTEGRATION_NAME}_integrated_umap.pdf\"),\n",
    "    bbox_inches='tight',\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Save individual spatial domain plots\n",
    "for sample_id in SAMPLE_IDS:\n",
    "    sample_data = combined_adata[combined_adata.obs[\"batch_name\"]==sample_id]\n",
    "    \n",
    "    spatial_plot = sc.pl.spatial(\n",
    "        sample_data,\n",
    "        color='DeepST_refine_domain',\n",
    "        title=f\"Spatial Domains - {sample_id}\",\n",
    "        frameon=False,\n",
    "        spot_size=150,\n",
    "        return_fig=True\n",
    "    )\n",
    "    spatial_plot.savefig(\n",
    "        os.path.join(RESULTS_DIR, f\"{sample_id}_domains.pdf\"),\n",
    "        bbox_inches='tight',\n",
    "        dpi=300\n",
    "    )\n",
    "\n",
    "print(f\"Integration complete! Results saved to {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
